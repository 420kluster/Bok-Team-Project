{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de987f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 토큰화 진행 중.\n",
      "\n",
      "10개 완료\n",
      "20개 완료\n",
      "30개 완료\n",
      "40개 완료\n",
      "50개 완료\n",
      "60개 완료\n",
      "70개 완료\n",
      "80개 완료\n",
      "90개 완료\n",
      "100개 완료\n",
      "110개 완료\n",
      "120개 완료\n",
      "130개 완료\n",
      "140개 완료\n",
      "150개 완료\n",
      "\n",
      ">>> 토큰화 완료.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# ==========================================\n",
    "# 1. 설정\n",
    "SOURCE_DIR = \"BOK_Cleaned\"\n",
    "TARGET_DIR = \"BOK_Tokenized\"\n",
    "# ==========================================\n",
    "\n",
    "if not os.path.exists(TARGET_DIR):\n",
    "    os.makedirs(TARGET_DIR)\n",
    "\n",
    "print(f\">>> 토큰화 진행 중.\\n\")\n",
    "\n",
    "# 형태소 분석기 준비 (Okt)\n",
    "okt = Okt()\n",
    "\n",
    "def extract_keywords_mecab_style(text):\n",
    "    # 1. 형태소 분석\n",
    "    pos_result = okt.pos(text, stem=True)\n",
    "    \n",
    "    keywords = []\n",
    "    for word, tag in pos_result:\n",
    "        if len(word) <= 1: # 1글자 제외\n",
    "            continue\n",
    "\n",
    "        new_tag = \"\"\n",
    "        \n",
    "        # 부정어 로직 변환\n",
    "        if word in ['않다', '못하다', '말다', '아니다']: \n",
    "            new_tag = 'VX'\n",
    "            word = word[:-1] \n",
    "        \n",
    "        # 일반 품사 매핑\n",
    "        elif tag == 'Noun':\n",
    "            new_tag = 'NNG'  # 명사\n",
    "        elif tag == 'Verb':\n",
    "            new_tag = 'VV'   # 동사\n",
    "        elif tag == 'Adjective':\n",
    "            new_tag = 'VA'   # 형용사\n",
    "        elif tag == 'Adverb':\n",
    "            new_tag = 'MAG'  # 부사\n",
    "        \n",
    "        # 매칭되는 태그가 있으면 추가\n",
    "        if new_tag != \"\":\n",
    "            keywords.append(f\"{word}/{new_tag}\")\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# 파일 처리 시작\n",
    "files = os.listdir(SOURCE_DIR)\n",
    "files.sort()\n",
    "count = 0\n",
    "\n",
    "for filename in files:\n",
    "    if not filename.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "        \n",
    "    source_path = os.path.join(SOURCE_DIR, filename)\n",
    "    target_path = os.path.join(TARGET_DIR, filename)\n",
    "    \n",
    "    try:\n",
    "        with open(source_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 변환 함수 실행\n",
    "        keywords = extract_keywords_mecab_style(content)\n",
    "        \n",
    "        # 저장\n",
    "        with open(target_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\",\".join(keywords))\n",
    "            \n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\"{count}개 완료\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"에러 ({filename}): {e}\")\n",
    "\n",
    "print(f\"\\n>>> 토큰화 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b4a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
